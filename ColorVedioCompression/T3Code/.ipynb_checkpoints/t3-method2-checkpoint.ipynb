{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 673181,
     "status": "error",
     "timestamp": 1584156173737,
     "user": {
      "displayName": "Lin P",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPffCWd2q_xshsU605IiPdrVLEo2gckbFVNS9I=s64",
      "userId": "16827509713046766681"
     },
     "user_tz": 420
    },
    "id": "0bI0saEe2V5U",
    "outputId": "8b566e51-233c-4bcd-efc6-a1a2c470313e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
      "You set: `1.13.1`. This will be interpreted as: `1.x`.\n",
      "\n",
      "\n",
      "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n",
      "800 200\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 120, 104, 32)      896       \n",
      "_________________________________________________________________\n",
      "gdn_1 (GDN)                  (None, 120, 104, 32)      1056      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 120, 52, 16)       4624      \n",
      "_________________________________________________________________\n",
      "gdn_2 (GDN)                  (None, 120, 52, 16)       272       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 120, 52, 8)        1160      \n",
      "_________________________________________________________________\n",
      "gdn_3 (GDN)                  (None, 120, 52, 8)        72        \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 120, 52, 4)        292       \n",
      "_________________________________________________________________\n",
      "gdn_4 (GDN)                  (None, 120, 52, 4)        20        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 120, 52, 8)        296       \n",
      "_________________________________________________________________\n",
      "gdn_5 (GDN)                  (None, 120, 52, 8)        72        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 120, 104, 16)      1168      \n",
      "_________________________________________________________________\n",
      "gdn_6 (GDN)                  (None, 120, 104, 16)      272       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 480, 416, 32)      4640      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 480, 416, 3)       867       \n",
      "=================================================================\n",
      "Total params: 15,707\n",
      "Trainable params: 15,707\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 120, 104, 32)      896       \n",
      "_________________________________________________________________\n",
      "gdn_7 (GDN)                  (None, 120, 104, 32)      1056      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 120, 104, 16)      4624      \n",
      "_________________________________________________________________\n",
      "gdn_8 (GDN)                  (None, 120, 104, 16)      272       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 120, 104, 8)       1160      \n",
      "_________________________________________________________________\n",
      "gdn_9 (GDN)                  (None, 120, 104, 8)       72        \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 120, 104, 4)       292       \n",
      "_________________________________________________________________\n",
      "gdn_10 (GDN)                 (None, 120, 104, 4)       20        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 120, 104, 8)       296       \n",
      "_________________________________________________________________\n",
      "gdn_11 (GDN)                 (None, 120, 104, 8)       72        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 120, 104, 16)      1168      \n",
      "_________________________________________________________________\n",
      "gdn_12 (GDN)                 (None, 120, 104, 16)      272       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 480, 416, 32)      4640      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 480, 416, 3)       867       \n",
      "=================================================================\n",
      "Total params: 15,707\n",
      "Trainable params: 15,707\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 240, 208, 32)      896       \n",
      "_________________________________________________________________\n",
      "gdn_13 (GDN)                 (None, 240, 208, 32)      1056      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 240, 104, 16)      4624      \n",
      "_________________________________________________________________\n",
      "gdn_14 (GDN)                 (None, 240, 104, 16)      272       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 240, 104, 8)       1160      \n",
      "_________________________________________________________________\n",
      "gdn_15 (GDN)                 (None, 240, 104, 8)       72        \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 240, 104, 4)       292       \n",
      "_________________________________________________________________\n",
      "gdn_16 (GDN)                 (None, 240, 104, 4)       20        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 240, 104, 8)       296       \n",
      "_________________________________________________________________\n",
      "gdn_17 (GDN)                 (None, 240, 104, 8)       72        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_9 (Conv2DTr (None, 240, 208, 16)      1168      \n",
      "_________________________________________________________________\n",
      "gdn_18 (GDN)                 (None, 240, 208, 16)      272       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_10 (Conv2DT (None, 480, 416, 32)      4640      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_11 (Conv2DT (None, 480, 416, 3)       867       \n",
      "=================================================================\n",
      "Total params: 15,707\n",
      "Trainable params: 15,707\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      "800/800 [==============================] - 262s 327ms/sample - loss: 8134.4378\n",
      "Epoch 2/30\n",
      "800/800 [==============================] - 265s 332ms/sample - loss: 1664.4507\n",
      "Epoch 3/30\n",
      "320/800 [===========>..................] - ETA: 2:39 - loss: 1340.2286"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-eb17bdb8931d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m    \u001b[0mrunALL_with_diff_cr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-eb17bdb8931d>\u001b[0m in \u001b[0;36mrunALL_with_diff_cr\u001b[0;34m()\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0mr16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcr_1_16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0mr8\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mcr_1_8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     \u001b[0mr4\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mcr_1_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m     \u001b[0mr2\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mcr_1_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-eb17bdb8931d>\u001b[0m in \u001b[0;36mcr_1_4\u001b[0;34m()\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestSet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.13.1\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "from PIL import Image\n",
    "import math\n",
    "import tensorflow.contrib.layers as ly\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# RaceHorses_416x240_30\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "videoName = 'Horse'\n",
    "epochs = 30\n",
    "basePath = Path(\"/content/gdrive/My Drive/Colab Notebooks/RaceHorses_416x240_30/\")\n",
    "# imgList = [np.array(Image.open(i).convert('RGB')) for i in basePath.iterdir()]\n",
    "\n",
    "imgList = []\n",
    "for i in basePath.iterdir():\n",
    "    if not i.is_dir():\n",
    "      x = np.array(Image.open(i).convert('RGB'))\n",
    "      imgList.append(x)\n",
    "\n",
    "\n",
    "\n",
    "numof_train = int(len(imgList)*0.8)\n",
    "\n",
    "numof_test = int(len(imgList) - numof_train)\n",
    "\n",
    "print(numof_train, numof_test)\n",
    "\n",
    "\n",
    "\n",
    "height = int(len(imgList[0]))\n",
    "length = int(len(imgList[0][0]))\n",
    "inputs = (height, length, 3)\n",
    "N = height * length\n",
    "\n",
    "\n",
    "train = imgList[:numof_train]  \n",
    "test = imgList[numof_train:] \n",
    "\n",
    "trainSet = np.array(train) \n",
    "testSet = np.array(test) \n",
    "\n",
    "def cr_1_32():\n",
    "    # compress\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), padding = \"SAME\", strides = (4, 4), activation='relu', input_shape=inputs)) \n",
    "    model.add(ly.GDN())\n",
    "    model.add(layers.Conv2D(16, (3, 3),  padding = \"SAME\", strides = (1, 2), activation='relu'))\n",
    "    model.add(ly.GDN())\n",
    "    model.add(layers.Conv2D(8, (3, 3),  padding = \"SAME\", strides = (1, 1), activation='relu'))\n",
    "    model.add(ly.GDN())\n",
    "    model.add(layers.Conv2D(4, (3, 3),  padding = \"SAME\", strides = (1, 1), activation='relu'))\n",
    "\n",
    "    # decompress \n",
    "    model.add(ly.GDN(inverse=True))\n",
    "    model.add(layers.Conv2DTranspose(8, (3, 3),   strides = (1, 1), padding = \"SAME\", activation='relu'))\n",
    "    model.add(ly.GDN(inverse=True))\n",
    "    model.add(layers.Conv2DTranspose(16, (3, 3),   strides = (1, 2), padding = \"SAME\", activation='relu'))\n",
    "    model.add(ly.GDN(inverse=True))\n",
    "    model.add(layers.Conv2DTranspose(32, (3, 3), strides = (4, 4),  padding = \"SAME\", activation='relu'))\n",
    "\n",
    "    # convert to 3 color channels \n",
    "    model.add(layers.Conv2DTranspose(3, (3, 3),  strides = (1, 1), padding = \"SAME\", activation='relu'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(trainSet, trainSet, epochs=epochs)\n",
    "    prediction = model.predict(testSet)\n",
    "\n",
    "    psnr = 0\n",
    "    for i in range(numof_test):\n",
    "        if i == 19 or i == 39 or i == 59:\n",
    "                colorlist = []\n",
    "                for m in range(height):\n",
    "                  for n in range(length):\n",
    "                    color = []\n",
    "                    for c in range(3):\n",
    "                      color.append( int(prediction[i][m][n][c]) )\n",
    "                    color = tuple(color)\n",
    "                    colorlist.append(color)\n",
    "\n",
    "                newImage = Image.new('RGB', (length, height))\n",
    "                newImage.putdata(colorlist)\n",
    "                newImage.save('/content/gdrive/My Drive/Colab Notebooks/output_Img/'+ videoName+\"_2_\"+str(i)+\"_1_32.png\")\n",
    "        img = test[i]\n",
    "        sum, mse = 0, 0\n",
    "        for m in range(height):\n",
    "            for n in range(length):\n",
    "                for color in range(3):\n",
    "                    sum += (img[m][n][color] - prediction[i][m][n][color])**2\n",
    "        mse = sum / (3 * N)\n",
    "        psnr += 10 * (math.log10(255*255/mse))\n",
    "\n",
    "    psnr = round(psnr / numof_test, 5)\n",
    "\n",
    "    return psnr\n",
    "\n",
    "\n",
    "def cr_1_16():\n",
    "    # compress\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), padding = \"SAME\", strides = (4, 4), activation='relu', input_shape=inputs))\n",
    "    model.add(ly.GDN())\n",
    "    model.add(layers.Conv2D(16, (3, 3),  padding = \"SAME\", strides = (1, 1), activation='relu'))\n",
    "    model.add(ly.GDN())\n",
    "    model.add(layers.Conv2D(8, (3, 3),  padding = \"SAME\", strides = (1, 1), activation='relu'))\n",
    "    model.add(ly.GDN())\n",
    "    model.add(layers.Conv2D(4, (3, 3),  padding = \"SAME\", strides = (1, 1), activation='relu'))\n",
    "\n",
    "    # decompress \n",
    "    model.add(ly.GDN(inverse=True))\n",
    "    model.add(layers.Conv2DTranspose(8, (3, 3),   strides = (1, 1), padding = \"SAME\", activation='relu'))\n",
    "    model.add(ly.GDN(inverse=True))\n",
    "    model.add(layers.Conv2DTranspose(16, (3, 3),   strides = (1, 1), padding = \"SAME\", activation='relu'))\n",
    "    model.add(ly.GDN(inverse=True))\n",
    "    model.add(layers.Conv2DTranspose(32, (3, 3), strides = (4, 4),  padding = \"SAME\", activation='relu'))\n",
    "\n",
    "    # convert to 3 color channels \n",
    "    model.add(layers.Conv2DTranspose(3, (3, 3),  strides = (1, 1), padding = \"SAME\", activation='relu'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(trainSet, trainSet, epochs=epochs)\n",
    "    prediction = model.predict(testSet)\n",
    "\n",
    "    psnr = 0\n",
    "    for i in range(numof_test):\n",
    "        if i == 19 or i == 39 or i == 59:\n",
    "              colorlist = []\n",
    "              for m in range(height):\n",
    "                for n in range(length):\n",
    "                  color = []\n",
    "                  for c in range(3):\n",
    "                    color.append( int(prediction[i][m][n][c]) )\n",
    "                  color = tuple(color)\n",
    "                  colorlist.append(color)\n",
    "\n",
    "              newImage = Image.new('RGB', (length, height))\n",
    "              newImage.putdata(colorlist)\n",
    "              newImage.save('/content/gdrive/My Drive/Colab Notebooks/output_Img/'+ videoName+\"_2_\"+str(i)+\"_1_16.png\")\n",
    "\n",
    "        img = test[i]\n",
    "        sum, mse = 0, 0\n",
    "        for m in range(height):\n",
    "            for n in range(length):\n",
    "                for color in range(3):\n",
    "                    sum += (img[m][n][color] - prediction[i][m][n][color])**2\n",
    "        mse = sum / (3 * N)\n",
    "        psnr += 10 * (math.log10(255*255/mse))\n",
    "\n",
    "    psnr = round(psnr / numof_test, 5)\n",
    "    return psnr\n",
    "\n",
    "\n",
    "def cr_1_8():\n",
    "    # compress\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), padding = \"SAME\", strides = (2, 2), activation='relu', input_shape=inputs)) \n",
    "    model.add(ly.GDN())\n",
    "    model.add(layers.Conv2D(16, (3, 3),  padding = \"SAME\", strides = (1, 2), activation='relu'))\n",
    "    model.add(ly.GDN())\n",
    "    model.add(layers.Conv2D(8, (3, 3),  padding = \"SAME\", strides = (1, 1), activation='relu'))\n",
    "    model.add(ly.GDN())\n",
    "    model.add(layers.Conv2D(4, (3, 3),  padding = \"SAME\", strides = (1, 1), activation='relu'))\n",
    "\n",
    "    # decompress \n",
    "    model.add(ly.GDN(inverse=True))\n",
    "    model.add(layers.Conv2DTranspose(8, (3, 3),   strides = (1, 1), padding = \"SAME\", activation='relu'))\n",
    "    model.add(ly.GDN(inverse=True))\n",
    "    model.add(layers.Conv2DTranspose(16, (3, 3),   strides = (1, 2), padding = \"SAME\", activation='relu'))\n",
    "    model.add(ly.GDN(inverse=True))\n",
    "    model.add(layers.Conv2DTranspose(32, (3, 3), strides = (2, 2),  padding = \"SAME\", activation='relu'))\n",
    "\n",
    "    # convert to 3 color channels \n",
    "    model.add(layers.Conv2DTranspose(3, (3, 3),  strides = (1, 1), padding = \"SAME\", activation='relu'))\n",
    "\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(trainSet, trainSet, epochs=epochs)\n",
    "    prediction = model.predict(testSet)\n",
    "\n",
    "    psnr = 0\n",
    "    for i in range(numof_test):\n",
    "        if i == 19 or i == 39 or i == 59:\n",
    "                colorlist = []\n",
    "                for m in range(height):\n",
    "                  for n in range(length):\n",
    "                    color = []\n",
    "                    for c in range(3):\n",
    "                      color.append( int(prediction[i][m][n][c]) )\n",
    "                    color = tuple(color)\n",
    "                    colorlist.append(color)\n",
    "\n",
    "                newImage = Image.new('RGB', (length, height))\n",
    "                newImage.putdata(colorlist)\n",
    "                newImage.save('/content/gdrive/My Drive/Colab Notebooks/output_Img/'+ videoName+\"_2_\"+str(i)+\"_1_8.png\")\n",
    "\n",
    "        img = test[i]\n",
    "        sum, mse = 0, 0\n",
    "        for m in range(height):\n",
    "            for n in range(length):\n",
    "                for color in range(3):\n",
    "                    sum += (img[m][n][color] - prediction[i][m][n][color])**2\n",
    "        mse = sum / (3 * N)\n",
    "        psnr += 10 * (math.log10(255*255/mse))\n",
    "\n",
    "\n",
    "    psnr = round(psnr / numof_test, 5)\n",
    "    return psnr\n",
    "\n",
    "\n",
    "def cr_1_4():\n",
    "    # compress\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), padding = \"SAME\", strides = (2, 2), activation='relu', input_shape=inputs)) \n",
    "    model.add(ly.GDN())\n",
    "    model.add(layers.Conv2D(16, (3, 3),  padding = \"SAME\", strides = (1, 1), activation='relu'))\n",
    "    model.add(ly.GDN())\n",
    "    model.add(layers.Conv2D(8, (3, 3),  padding = \"SAME\", strides = (1, 1), activation='relu'))\n",
    "    model.add(ly.GDN())\n",
    "    model.add(layers.Conv2D(4, (3, 3),  padding = \"SAME\", strides = (1, 1), activation='relu'))\n",
    "\n",
    "    # decompress \n",
    "    model.add(ly.GDN(inverse=True))\n",
    "    model.add(layers.Conv2DTranspose(8, (3, 3),   strides = (1, 1), padding = \"SAME\", activation='relu'))\n",
    "    model.add(ly.GDN(inverse=True))\n",
    "    model.add(layers.Conv2DTranspose(16, (3, 3),   strides = (1, 1), padding = \"SAME\", activation='relu'))\n",
    "    model.add(ly.GDN(inverse=True))\n",
    "    model.add(layers.Conv2DTranspose(32, (3, 3), strides = (2, 2),  padding = \"SAME\", activation='relu'))\n",
    "\n",
    "    # convert to 3 color channels \n",
    "    model.add(layers.Conv2DTranspose(3, (3, 3),  strides = (1, 1), padding = \"SAME\", activation='relu'))\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(trainSet, trainSet, epochs=epochs)\n",
    "    prediction = model.predict(testSet)\n",
    "\n",
    "\n",
    "    psnr = 0\n",
    "    for i in range(numof_test):\n",
    "        if i == 19 or i == 39 or i == 59:\n",
    "              colorlist = []\n",
    "              for m in range(height):\n",
    "                for n in range(length):\n",
    "                  color = []\n",
    "                  for c in range(3):\n",
    "                    color.append( int(prediction[i][m][n][c]) )\n",
    "                  color = tuple(color)\n",
    "                  colorlist.append(color)\n",
    "\n",
    "              newImage = Image.new('RGB', (length, height))\n",
    "              newImage.putdata(colorlist)\n",
    "              newImage.save('/content/gdrive/My Drive/Colab Notebooks/output_Img/'+ videoName+\"_2_\"+str(i)+\"_1_4.png\")\n",
    "\n",
    "\n",
    "        img = test[i]\n",
    "        sum, mse = 0, 0\n",
    "        for m in range(height):\n",
    "            for n in range(length):\n",
    "                for color in range(3):\n",
    "                    sum += (img[m][n][color] - prediction[i][m][n][color])**2\n",
    "        mse = sum / (3 * N)\n",
    "        psnr += 10 * (math.log10(255*255/mse))\n",
    "\n",
    "    psnr = round(psnr / numof_test, 5)\n",
    "    return psnr\n",
    "\n",
    "\n",
    "def cr_1_2():\n",
    "    # compress\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), padding = \"SAME\", strides = (1, 2), activation='relu', input_shape=inputs)) \n",
    "    model.add(ly.GDN())\n",
    "    model.add(layers.Conv2D(16, (3, 3),  padding = \"SAME\", strides = (1, 1), activation='relu'))\n",
    "    model.add(ly.GDN())\n",
    "    model.add(layers.Conv2D(8, (3, 3),  padding = \"SAME\", strides = (1, 1), activation='relu'))\n",
    "    model.add(ly.GDN())\n",
    "    model.add(layers.Conv2D(4, (3, 3),  padding = \"SAME\", strides = (1, 1), activation='relu'))\n",
    "\n",
    "    # decompress \n",
    "    model.add(ly.GDN(inverse=True))\n",
    "    model.add(layers.Conv2DTranspose(8, (3, 3),   strides = (1, 1), padding = \"SAME\", activation='relu'))\n",
    "    model.add(ly.GDN(inverse=True))\n",
    "    model.add(layers.Conv2DTranspose(16, (3, 3),   strides = (1, 1), padding = \"SAME\", activation='relu'))\n",
    "    model.add(ly.GDN(inverse=True))\n",
    "    model.add(layers.Conv2DTranspose(32, (3, 3), strides = (1, 2),  padding = \"SAME\", activation='relu'))\n",
    "\n",
    "    # convert to 3 color channels \n",
    "    model.add(layers.Conv2DTranspose(3, (3, 3),  strides = (1, 1), padding = \"SAME\", activation='relu'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(trainSet, trainSet, epochs=epochs)\n",
    "    prediction = model.predict(testSet)\n",
    "\n",
    "    psnr = 0\n",
    "    for i in range(numof_test):\n",
    "        if i == 19 or i == 39 or i == 59:\n",
    "                colorlist = []\n",
    "                for m in range(height):\n",
    "                  for n in range(length):\n",
    "                    color = []\n",
    "                    for c in range(3):\n",
    "                      color.append( int(prediction[i][m][n][c]) )\n",
    "                    color = tuple(color)\n",
    "                    colorlist.append(color)\n",
    "\n",
    "                newImage = Image.new('RGB', (length, height))\n",
    "                newImage.putdata(colorlist)\n",
    "                newImage.save('/content/gdrive/My Drive/Colab Notebooks/output_Img/'+ videoName+\"_2_\"+str(i)+\"_1_2.png\")\n",
    "        img = test[i]\n",
    "        sum, mse = 0, 0\n",
    "        for m in range(height):\n",
    "            for n in range(length):\n",
    "                for color in range(3):\n",
    "                    sum += (img[m][n][color] - prediction[i][m][n][color])**2\n",
    "        mse = sum / (3 * N)\n",
    "        psnr += 10 * (math.log10(255*255/mse))\n",
    "\n",
    "\n",
    "    psnr = round(psnr / numof_test, 5)\n",
    "    return psnr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def runALL_with_diff_cr():\n",
    "    r32 = cr_1_32()\n",
    "    r16 = cr_1_16()\n",
    "    r8  = cr_1_8()\n",
    "    r4  = cr_1_4()\n",
    "    r2  = cr_1_2()\n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    X  = [\"1/2\", \"1/4\", \"1/8\", \"1/16\", \"1/32\"]\n",
    "    Y1 = [r2, r4, r8, r16, r32]\n",
    "\n",
    "    plt.plot(X, Y1, color= '#1774FF', linewidth = 1, \n",
    "            marker='o', markerfacecolor='#1774FF', markersize=7) \n",
    "\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], marker='o', color='#1774FF', label= videoName,\n",
    "                          markerfacecolor='#1774FF', markersize=7)]\n",
    "\n",
    "    plt.legend(handles=legend_elements, loc='top right')\n",
    "    plt.xticks(X)\n",
    "    plt.xlabel('Compression Ratio') \n",
    "    plt.ylabel('PSNR') \n",
    "    plt.title('Outcome of ' + videoName + ' Video') \n",
    "    plt.show()\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"__Method2__\")\n",
    "    print(\"CR = 1/2, PSNR_with_\", epochs, \"Epochs = \", r2)\n",
    "    print(\"CR = 1/4, PSNR_with_\", epochs, \"Epochs = \", r4)\n",
    "    print(\"CR = 1/8, PSNR_with_\", epochs, \"Epochs = \", r8)\n",
    "    print(\"CR = 1/16, PSNR_with_\", epochs, \"Epochs = \", r16)\n",
    "    print(\"CR = 1/32, PSNR_with_\", epochs, \"Epochs = \", r32)\n",
    "    print(\"\")\n",
    "\n",
    "    return \n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "   runALL_with_diff_cr()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1120,
     "status": "ok",
     "timestamp": 1584155495952,
     "user": {
      "displayName": "Lin P",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPffCWd2q_xshsU605IiPdrVLEo2gckbFVNS9I=s64",
      "userId": "16827509713046766681"
     },
     "user_tz": 420
    },
    "id": "wA5fTkNg6r_o",
    "outputId": "5d46bf66-160e-4b7c-d0fb-3e8df12521cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
      "You set: `1.13.1`. This will be interpreted as: `1.x`.\n",
      "\n",
      "\n",
      "TensorFlow 1.x selected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%tensorflow_version 1.13.1\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "\n",
    "\n",
    "# !pip install tensorflow==1.13.1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOLpT9SN7ginAp0zDRKWTaP",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "mount_file_id": "1N0gIaBLfVO2re7FaiYPB4vOyz1mEDY7W",
   "name": "t3-method2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
